{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import rankdata\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING THE TRAINING FILE\n",
    "\n",
    "df_train = pd.read_csv(\"C:\\\\Users\\\\sivac\\\\Documents\\\\Python Projects\\\\Banco Santander Kaggle\\\\input\\\\train.csv\")\n",
    "df_test = pd.read_csv(\"C:\\\\Users\\\\sivac\\\\Documents\\\\Python Projects\\\\Banco Santander Kaggle\\\\input\\\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'target'\n",
    "predictors = df_train.columns.values.tolist()[2:]\n",
    "df_train.target.value_counts() # The problem is imbalanced as only around 10% of target is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% of the data set will be held for validation to set optimal parameters, and then 5 fold CV \n",
    "# for final model\n",
    "\n",
    "bayesian_tr_index, bayesian_val_index = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=2).split(df_train, df_train.target.values))[0]\n",
    "\n",
    "# The indices will be later used for the bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for optimizing Light GBM\n",
    "\n",
    "def lgb_parameter_optimization(\n",
    "    num_leaves, # int\n",
    "    min_data_in_leaf, # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf, # int\n",
    "    feature_fraction,\n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects the following variables in int. So, lets initialize them as integer\n",
    "    \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "    \n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'bagging_fraction': 1.0,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,  \n",
    "    }\n",
    "    \n",
    "    xg_train = lgb.Dataset(df_train.iloc[bayesian_tr_index][predictors].values,\n",
    "                          label=df_train.iloc[bayesian_tr_index][target].values,\n",
    "                          feature_name=predictors,\n",
    "                          free_raw_data = False\n",
    "                          )\n",
    "    xg_validation = lgb.Dataset(df_train.iloc[bayesian_val_index][predictors].values,\n",
    "                          label=df_train.iloc[bayesian_val_index][target].values,\n",
    "                          feature_name=predictors,\n",
    "                          free_raw_data = False\n",
    "                          )\n",
    "    num_round=5000\n",
    "    model = lgb.train(param, xg_train, num_round, valid_sets = [xg_validation], verbose_eval=250,\n",
    "                   early_stopping_rounds=60)\n",
    "    predictions = model.predict(df_train.iloc[bayesian_val_index][predictors].values, \n",
    "                                num_iteration=model.best_iteration)\n",
    "    score = metrics.roc_auc_score(df_train.iloc[bayesian_val_index][target].values, predictions)\n",
    "    return score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LGB_bayesian function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the LGB_bayesian function.\n",
    "\n",
    "The LGB_bayesian function takes values for num_leaves, min_data_in_leaf, learning_rate, min_sum_hessian_in_leaf, feature_fraction, lambda_l1, lambda_l2, min_gain_to_split, max_depth from Bayesian optimization framework. Keep in mind that num_leaves, min_data_in_leaf, and max_depth should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize.\n",
    "\n",
    "Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (5, 20), \n",
    "    'min_data_in_leaf': (5, 20),  \n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n",
    "    'feature_fraction': (0.05, 0.5),\n",
    "    'lambda_l1': (0, 5.0), \n",
    "    'lambda_l2': (0, 5.0), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(3,15),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now include everythingg in the optimization function\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "LGB_BO = BayesianOptimization(lgb_parameter_optimization, bounds_LGB, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's the the key space (parameters) we are going to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_fraction', 'lambda_l1', 'lambda_l2', 'learning_rate', 'max_depth', 'min_data_in_leaf', 'min_gain_to_split', 'min_sum_hessian_in_leaf', 'num_leaves']\n"
     ]
    }
   ],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created the BayesianOptimization object (LGB_BO), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (LGB_BO) which we can pass to maximize:\n",
    "\n",
    "init_points: How many initial random runs of random exploration we want to perform. In our case LGB_bayesian will be called n_iter times.\n",
    "n_iter: How many runs of bayesian optimization we want to perform after number of init_points runs.\n",
    "Now, it's time to call the function from Bayesian optimization framework to maximize. I allow LGB_BO object to run for 5 init_points (exploration) and 5 n_iter (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.87519\n",
      "[500]\tvalid_0's auc: 0.888834\n",
      "[750]\tvalid_0's auc: 0.891982\n",
      "Early stopping, best iteration is:\n",
      "[876]\tvalid_0's auc: 0.892315\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8923  \u001b[0m | \u001b[0m 0.2925  \u001b[0m | \u001b[0m 2.096   \u001b[0m | \u001b[0m 3.426   \u001b[0m | \u001b[0m 0.06929 \u001b[0m | \u001b[0m 13.54   \u001b[0m | \u001b[0m 5.411   \u001b[0m | \u001b[0m 0.6705  \u001b[0m | \u001b[0m 0.004179\u001b[0m | \u001b[0m 13.38   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.888477\n",
      "Early stopping, best iteration is:\n",
      "[324]\tvalid_0's auc: 0.889685\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8897  \u001b[0m | \u001b[0m 0.1132  \u001b[0m | \u001b[0m 0.9905  \u001b[0m | \u001b[0m 4.004   \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 6.761   \u001b[0m | \u001b[0m 15.38   \u001b[0m | \u001b[0m 0.8764  \u001b[0m | \u001b[0m 0.008947\u001b[0m | \u001b[0m 6.276   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.865228\n",
      "[500]\tvalid_0's auc: 0.878349\n",
      "[750]\tvalid_0's auc: 0.885688\n",
      "[1000]\tvalid_0's auc: 0.890583\n",
      "[1250]\tvalid_0's auc: 0.893327\n",
      "[1500]\tvalid_0's auc: 0.89505\n",
      "[1750]\tvalid_0's auc: 0.895916\n",
      "[2000]\tvalid_0's auc: 0.896359\n",
      "Early stopping, best iteration is:\n",
      "[2067]\tvalid_0's auc: 0.896448\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8964  \u001b[0m | \u001b[95m 0.06757 \u001b[0m | \u001b[95m 0.8492  \u001b[0m | \u001b[95m 4.391   \u001b[0m | \u001b[95m 0.03852 \u001b[0m | \u001b[95m 8.053   \u001b[0m | \u001b[95m 19.37   \u001b[0m | \u001b[95m 0.5332  \u001b[0m | \u001b[95m 0.006922\u001b[0m | \u001b[95m 9.733   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.886198\n",
      "[500]\tvalid_0's auc: 0.889771\n",
      "Early stopping, best iteration is:\n",
      "[440]\tvalid_0's auc: 0.890293\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.3589  \u001b[0m | \u001b[0m 4.173   \u001b[0m | \u001b[0m 0.09144 \u001b[0m | \u001b[0m 0.2275  \u001b[0m | \u001b[0m 14.87   \u001b[0m | \u001b[0m 16.22   \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 0.007895\u001b[0m | \u001b[0m 6.548   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.871614\n",
      "[500]\tvalid_0's auc: 0.886927\n",
      "[750]\tvalid_0's auc: 0.89165\n",
      "[1000]\tvalid_0's auc: 0.892472\n",
      "Early stopping, best iteration is:\n",
      "[1085]\tvalid_0's auc: 0.892601\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8926  \u001b[0m | \u001b[0m 0.2516  \u001b[0m | \u001b[0m 4.543   \u001b[0m | \u001b[0m 1.468   \u001b[0m | \u001b[0m 0.09345 \u001b[0m | \u001b[0m 4.56    \u001b[0m | \u001b[0m 5.291   \u001b[0m | \u001b[0m 0.6788  \u001b[0m | \u001b[0m 0.002124\u001b[0m | \u001b[0m 8.983   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.872007\n",
      "[500]\tvalid_0's auc: 0.887782\n",
      "[750]\tvalid_0's auc: 0.892459\n",
      "[1000]\tvalid_0's auc: 0.893693\n",
      "Early stopping, best iteration is:\n",
      "[1003]\tvalid_0's auc: 0.893733\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8937  \u001b[0m | \u001b[0m 0.2712  \u001b[0m | \u001b[0m 0.2668  \u001b[0m | \u001b[0m 2.871   \u001b[0m | \u001b[0m 0.05255 \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 15.5    \u001b[0m | \u001b[0m 0.1023  \u001b[0m | \u001b[0m 0.004146\u001b[0m | \u001b[0m 15.42   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.887006\n",
      "Early stopping, best iteration is:\n",
      "[387]\tvalid_0's auc: 0.89034\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.2364  \u001b[0m | \u001b[0m 0.2498  \u001b[0m | \u001b[0m 2.679   \u001b[0m | \u001b[0m 0.2025  \u001b[0m | \u001b[0m 9.179   \u001b[0m | \u001b[0m 19.17   \u001b[0m | \u001b[0m 0.5866  \u001b[0m | \u001b[0m 0.009035\u001b[0m | \u001b[0m 7.062   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.877873\n",
      "[500]\tvalid_0's auc: 0.890371\n",
      "[750]\tvalid_0's auc: 0.894045\n",
      "[1000]\tvalid_0's auc: 0.894872\n",
      "Early stopping, best iteration is:\n",
      "[1035]\tvalid_0's auc: 0.895058\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8951  \u001b[0m | \u001b[0m 0.1127  \u001b[0m | \u001b[0m 4.037   \u001b[0m | \u001b[0m 1.988   \u001b[0m | \u001b[0m 0.05795 \u001b[0m | \u001b[0m 14.13   \u001b[0m | \u001b[0m 10.22   \u001b[0m | \u001b[0m 0.7508  \u001b[0m | \u001b[0m 0.007263\u001b[0m | \u001b[0m 18.25   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.88141\n",
      "[500]\tvalid_0's auc: 0.890833\n",
      "Early stopping, best iteration is:\n",
      "[567]\tvalid_0's auc: 0.891095\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8911  \u001b[0m | \u001b[0m 0.3307  \u001b[0m | \u001b[0m 3.755   \u001b[0m | \u001b[0m 1.744   \u001b[0m | \u001b[0m 0.08828 \u001b[0m | \u001b[0m 13.75   \u001b[0m | \u001b[0m 11.42   \u001b[0m | \u001b[0m 0.9648  \u001b[0m | \u001b[0m 0.006638\u001b[0m | \u001b[0m 14.33   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.862757\n",
      "[500]\tvalid_0's auc: 0.878901\n",
      "[750]\tvalid_0's auc: 0.88704\n",
      "[1000]\tvalid_0's auc: 0.890593\n",
      "[1250]\tvalid_0's auc: 0.893233\n",
      "[1500]\tvalid_0's auc: 0.894722\n",
      "[1750]\tvalid_0's auc: 0.895621\n",
      "[2000]\tvalid_0's auc: 0.895881\n",
      "Early stopping, best iteration is:\n",
      "[1953]\tvalid_0's auc: 0.895959\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.896   \u001b[0m | \u001b[0m 0.05409 \u001b[0m | \u001b[0m 4.228   \u001b[0m | \u001b[0m 4.73    \u001b[0m | \u001b[0m 0.06452 \u001b[0m | \u001b[0m 3.053   \u001b[0m | \u001b[0m 19.21   \u001b[0m | \u001b[0m 0.2128  \u001b[0m | \u001b[0m 0.00421 \u001b[0m | \u001b[0m 19.83   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.799455\n",
      "[500]\tvalid_0's auc: 0.833199\n",
      "[750]\tvalid_0's auc: 0.849677\n",
      "[1000]\tvalid_0's auc: 0.859829\n",
      "[1250]\tvalid_0's auc: 0.867081\n",
      "[1500]\tvalid_0's auc: 0.872361\n",
      "[1750]\tvalid_0's auc: 0.876703\n",
      "[2000]\tvalid_0's auc: 0.879976\n",
      "[2250]\tvalid_0's auc: 0.882522\n",
      "[2500]\tvalid_0's auc: 0.884511\n",
      "[2750]\tvalid_0's auc: 0.886169\n",
      "[3000]\tvalid_0's auc: 0.887586\n",
      "[3250]\tvalid_0's auc: 0.88866\n",
      "[3500]\tvalid_0's auc: 0.889699\n",
      "[3750]\tvalid_0's auc: 0.890483\n",
      "[4000]\tvalid_0's auc: 0.891084\n",
      "[4250]\tvalid_0's auc: 0.89175\n",
      "[4500]\tvalid_0's auc: 0.892249\n",
      "[4750]\tvalid_0's auc: 0.892722\n",
      "[5000]\tvalid_0's auc: 0.893027\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\tvalid_0's auc: 0.893029\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.893   \u001b[0m | \u001b[0m 0.3008  \u001b[0m | \u001b[0m 0.07528 \u001b[0m | \u001b[0m 0.08172 \u001b[0m | \u001b[0m 0.01769 \u001b[0m | \u001b[0m 3.257   \u001b[0m | \u001b[0m 19.38   \u001b[0m | \u001b[0m 0.2729  \u001b[0m | \u001b[0m 0.004495\u001b[0m | \u001b[0m 19.44   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.881291\n",
      "[500]\tvalid_0's auc: 0.887621\n",
      "Early stopping, best iteration is:\n",
      "[618]\tvalid_0's auc: 0.888418\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8884  \u001b[0m | \u001b[0m 0.3289  \u001b[0m | \u001b[0m 4.64    \u001b[0m | \u001b[0m 4.753   \u001b[0m | \u001b[0m 0.2126  \u001b[0m | \u001b[0m 3.734   \u001b[0m | \u001b[0m 19.09   \u001b[0m | \u001b[0m 0.3767  \u001b[0m | \u001b[0m 0.001444\u001b[0m | \u001b[0m 19.85   \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.790554\n",
      "[500]\tvalid_0's auc: 0.825334\n",
      "[750]\tvalid_0's auc: 0.843238\n",
      "[1000]\tvalid_0's auc: 0.854193\n",
      "[1250]\tvalid_0's auc: 0.861733\n",
      "[1500]\tvalid_0's auc: 0.867301\n",
      "[1750]\tvalid_0's auc: 0.871903\n",
      "[2000]\tvalid_0's auc: 0.875583\n",
      "[2250]\tvalid_0's auc: 0.878658\n",
      "[2500]\tvalid_0's auc: 0.881143\n",
      "[2750]\tvalid_0's auc: 0.883106\n",
      "[3000]\tvalid_0's auc: 0.884795\n",
      "[3250]\tvalid_0's auc: 0.88618\n",
      "[3500]\tvalid_0's auc: 0.887335\n",
      "[3750]\tvalid_0's auc: 0.888393\n",
      "[4000]\tvalid_0's auc: 0.889243\n",
      "[4250]\tvalid_0's auc: 0.890012\n",
      "[4500]\tvalid_0's auc: 0.890781\n",
      "[4750]\tvalid_0's auc: 0.891436\n",
      "[5000]\tvalid_0's auc: 0.891959\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's auc: 0.891959\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.892   \u001b[0m | \u001b[0m 0.2218  \u001b[0m | \u001b[0m 0.4271  \u001b[0m | \u001b[0m 3.764   \u001b[0m | \u001b[0m 0.01472 \u001b[0m | \u001b[0m 3.846   \u001b[0m | \u001b[0m 5.065   \u001b[0m | \u001b[0m 0.06501 \u001b[0m | \u001b[0m 0.006192\u001b[0m | \u001b[0m 19.4    \u001b[0m |\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[250]\tvalid_0's auc: 0.885243\n",
      "[500]\tvalid_0's auc: 0.892728\n",
      "Early stopping, best iteration is:\n",
      "[687]\tvalid_0's auc: 0.893187\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8932  \u001b[0m | \u001b[0m 0.1207  \u001b[0m | \u001b[0m 1.503   \u001b[0m | \u001b[0m 4.12    \u001b[0m | \u001b[0m 0.09249 \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 19.85   \u001b[0m | \u001b[0m 0.3042  \u001b[0m | \u001b[0m 0.009807\u001b[0m | \u001b[0m 18.63   \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8964483321228044"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the max AUC score\n",
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.06757465245479707,\n",
       " 'lambda_l1': 0.8491520978228445,\n",
       " 'lambda_l2': 4.390712517147065,\n",
       " 'learning_rate': 0.03852058181158453,\n",
       " 'max_depth': 8.053291500060626,\n",
       " 'min_data_in_leaf': 19.368342952257528,\n",
       " 'min_gain_to_split': 0.5331652849730171,\n",
       " 'min_sum_hessian_in_leaf': 0.006921852368365229,\n",
       " 'num_leaves': 9.732734465090944}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the parameters responsible for this max score\n",
    "\n",
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/fayzur/lgb-bayesian-parameters-finding-rank-average\n",
    "# https://www.kaggle.com/fayzur/lightgbm-customer-transaction-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
